{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117138a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for inputs\n",
    "\"\"\"\n",
    "We are going to test all of the different models in this notebook with different groups of cells representing each model\n",
    "\"\"\"\n",
    "# Few-shot\n",
    "# input = \"Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output:\"\n",
    "# Chain of Thought\n",
    "input = \"Context: Consequently, further experiments were carried out while using current density of 4.27 mA. cm–2, and changes were applied to the concentrations of bath compounds which consist of 8.5 g. l –1 AgCN, 8.5 g. l –1 KCN, 3 g. l –1 Na2CO3. After changing the concentrations of the bath, the influence of acidity on current efficiency was concerned in further experiments. These alternations had stopped the undesired production of chalk-white sediment on the surface.\\n Question: What elements make up the molecules above? \\n Answer: AgCN, KCN, and Na2CO3 are molecules. Ag is the atomic symbol for gold. C is the atomic symbol for carbon. N is the atomic symbol for nitrogen. Na is the atomic symbol for sodium. O is the atomic symbol for oxygen. So gold, carbon, nitrogen, sodium, and oxygen make up the molecules above.\\n Context: Natural thallium(I) sulfate (Tl2SO4), was chosen as the target material. Highquality thallium targets (70 μm) were prepared by electrodeposition of 1 g of Tl on 12 cm2 copper backings. The first plating bath is prepared by dissolving a portion of natTl2SO4 in an aqueous alkaline solution containing EDTA as a complexing agent, hydrazine hydrate as an anodic depolarizer, and Brij-35 as a wetting agent.\\n Question: What elements make up the molecules in the above context?\\n Answer: \"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86219945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 12 11:35:09 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:65:00.0 Off |                  Off |\r\n",
      "|  0%   51C    P8              38W / 450W |    420MiB / 24564MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A    175299      G   /usr/lib/xorg/Xorg                          278MiB |\r\n",
      "|    0   N/A  N/A    175584      G   /usr/bin/gnome-shell                         13MiB |\r\n",
      "|    0   N/A  N/A    176476      G   ...AAAAAAAACAAAAAAAAAA= --shared-files       31MiB |\r\n",
      "|    0   N/A  N/A    192783      G   ...71917030,7979034060199083169,262144       77MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 1374106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18265de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaTokenizer' from 'transformers' (/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# OpenLLaMA runs into an error trying to import LlamaTokenizer and LlamaForCausalLM, so we're moving on to the next\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# After reading I have discovered that LlamaTokenizer and LlamaForCausalLM are only obtainable \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# by getting LLaMA's weights and converting them to HuggingFace format.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer, LlamaForCausalLM\n\u001b[1;32m      7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenlm-research/open_llama_3b\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model_path = 'openlm-research/open_llama_7b'\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaTokenizer' from 'transformers' (/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# OpenLLaMA runs into an error trying to import LlamaTokenizer and LlamaForCausalLM, so we're moving on to the next\n",
    "# After reading I have discovered that LlamaTokenizer and LlamaForCausalLM are only obtainable \n",
    "# by getting LLaMA's weights and converting them to HuggingFace format.\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_length=400\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47897b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Context: Consequently, further experiments were carried out while using current density of 4.27 mA. cm–2, and changes were applied to the concentrations of bath compounds which consist of 8.5 g. l –1 AgCN, 8.5 g. l –1 KCN, 3 g. l –1 Na2CO3. After changing the concentrations of the bath, the influence of acidity on current efficiency was concerned in further experiments. These alternations had stopped the undesired production of chalk-white sediment on the surface.\\n Question: What elements make up the molecules above? \\n Answer: AgCN, KCN, and Na2CO3 are molecules. Ag is the atomic symbol for gold. C is the atomic symbol for carbon. N is the atomic symbol for nitrogen. Na is the atomic symbol for sodium. O is the atomic symbol for oxygen. So gold, carbon, nitrogen, sodium, and oxygen make up the molecules above.\\n Context: Natural thallium(I) sulfate (Tl2SO4), was chosen as the target material. Highquality thallium targets (70 μm) were prepared by electrodeposition of 1 g of Tl on 12 cm2 copper backings. The first plating bath is prepared by dissolving a portion of natTl2SO4 in an aqueous alkaline solution containing EDTA as a complexing agent, hydrazine hydrate as an anodic depolarizer, and Brij-35 as a wetting agent.\\n Question: What elements make up the molecules in the above context?\\n Answer:  AgCN, KCN, and Na2CO3 are molecules. Ag is the atomic symbol for gold. C is the atomic symbol for carbon. N is the atomic symbol for nitrogen. Na is the atomic symbol for sodium. O is the atomic symbol for oxygen. So gold, carbon, nitrogen, sodium, and oxygen make up the'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPT-6.7B\n",
    "# Currently it keeps repeating the answer that was given in the previous example\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "model = pipeline('text-generation', model=\"facebook/opt-6.7b\")\n",
    "model(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPT-2.7B\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "model = pipeline('text-generation', model=\"facebook/opt-2.7b\")\n",
    "model(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6b113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023d4b6f8d374e98a398dbafbd09263a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdbab3f5a814087b34063a72b26b97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02864bb46a9448bb818d7c64e673211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc54d69477e442fa864b97ed5a13881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am(OH)3, CaCO3, Na2CO3, K2CO3, Mg2CO3, Ca(OH)2, Mg(OH)2, Ca(OH)2, Mg(OH)2, Ca(OH)2, Mg(OH)2, Ca(OH)2, Mg(OH)2, Ca(OH)'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPT-1.3b\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "model = pipeline('text-generation', model=\"facebook/opt-1.3b\")\n",
    "model(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1f5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/generation/utils.py:1470: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Cations and anions, anions and cations, and anions and cations Answer: The most abundant elements present in the oceans are the following (with relative abundance in parenthesis): Al, As, Br, Cu, Fe, Mg, Na, Pb, S, Sr, Ti, Tl, and Zn Question: What type of molecules would be present if the ocean were a\n"
     ]
    }
   ],
   "source": [
    "# Falcon-RW-1B\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "    input,\n",
    "    max_length=400,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a7da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/home/lendes/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "['Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: AnO2 +, PuO2 +, NpO2 +, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)']\n"
     ]
    }
   ],
   "source": [
    "# MPT-1B-Redpajama-200B-Dolly\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-1b-redpajama-200b-dolly', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('mosaicml/mpt-1b-redpajama-200b-dolly', trust_remote_code=True)\n",
    "model.to(device='cuda:0')\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\").to(device='cuda:0')\n",
    "outputs = model.generate(**inputs, max_length=400)\n",
    "text_output = tokenizer.batch_decode(outputs)\n",
    "print(text_output)\n",
    "#print(text_output[0].split(\"<|endoftext|>\")[0].split(input)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f4a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash_attn\n",
      "  Using cached flash_attn-1.0.8.tar.gz (2.0 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-ynnwxd__/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 341, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-ynnwxd__/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 323, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-ynnwxd__/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 338, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 13, in <module>\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install flash_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300c8ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e767c3c05eeb4ef4a27ffaa6bf79cd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1302c798ee4dfb92ef1612eb3f8d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)configuration_mpt.py:   0%|          | 0.00/9.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9bb87263944d8dbdbbaa230fa8be2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)main/modeling_mpt.py:   0%|          | 0.00/19.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b0610ec6b144beb64d82afc6175733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901d4557364c4393930b3b4ab5fd9093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)refixlm_converter.py:   0%|          | 0.00/27.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa11217970e4680a05ca5e4d461c4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)meta_init_context.py:   0%|          | 0.00/3.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8a9b35ece34d26a46c16d2c60b87c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/custom_embedding.py:   0%|          | 0.00/305 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350ac3e5b6414aadabef106ef0c7d059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)resolve/main/norm.py:   0%|          | 0.00/2.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c49387892a44aca50bc3bf1be82c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/attention.py:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: einops, flash_attn. Run `pip install einops flash_attn`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MPT-7B-Instruct\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmosaicml/mpt-7b-instruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:455\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[1;32m    454\u001b[0m     module_file, class_name \u001b[38;5;241m=\u001b[39m class_ref\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 455\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    459\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/dynamic_module_utils.py:363\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(pretrained_model_name_or_path, module_file, class_name, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03mExtracts a class from a module file, present in the local folder or repository of a model.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03mcls = get_class_from_dynamic_module(\"sgugger/my-bert-model\", \"modeling.py\", \"MyBertModel\")\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/dynamic_module_utils.py:274\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_needed \u001b[38;5;129;01min\u001b[39;00m modules_needed:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (submodule_path \u001b[38;5;241m/\u001b[39m module_needed)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 274\u001b[0m             \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule_needed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(full_submodule, module_file)\n",
      "File \u001b[0;32m~/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/dynamic_module_utils.py:237\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
      "File \u001b[0;32m~/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/dynamic_module_utils.py:134\u001b[0m, in \u001b[0;36mcheck_imports\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    131\u001b[0m         missing_packages\u001b[38;5;241m.\u001b[39mappend(imp)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[0;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: einops, flash_attn. Run `pip install einops flash_attn`"
     ]
    }
   ],
   "source": [
    "# MPT-7B-Instruct\n",
    "# Running into an error install flash_attn, see https://github.com/HazyResearch/flash-attention \n",
    "# and https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch\n",
    "# to set up a container that can successfully install it (hopefully able to run in a conda env and jupyter)\n",
    "import transformers\n",
    "import einops\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  'mosaicml/mpt-7b-instruct',\n",
    "  trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.generate(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab24839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: AnO2 +, NpO2 +, PuO2 +, Pu(OH)4, Pu(OH)4 (pKsp56), Pu(OH)4 (pKsp57), Pu(OH)4 (p'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Galactica - base (1.3b model)\n",
    "import torch\n",
    "import galai\n",
    "\n",
    "model = galai.load_model(\"base\")\n",
    "model.generate(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9baedacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context: Consequently, further experiments were carried out while using current density of 4.27 mA. cm–2, and changes were applied to the concentrations of bath compounds which consist of 8.5 g. l –1 AgCN, 8.5 g. l –1 KCN, 3 g. l –1 Na2CO3. After changing the concentrations of the bath, the influence of acidity on current efficiency was concerned in further experiments. These alternations had stopped the undesired production of chalk-white sediment on the surface.\\n Question: What elements make up the molecules above? \\n Answer: AgCN, KCN, and Na2CO3 are molecules. Ag is the atomic symbol for gold. C is the atomic symbol for carbon. N is the atomic symbol for nitrogen. Na is the atomic symbol for sodium. O is the atomic symbol for oxygen. So gold, carbon, nitrogen, sodium, and oxygen make up the molecules above.\\n Context: Natural thallium(I) sulfate (Tl2SO4), was chosen as the target material. Highquality thallium targets (70 μm) were prepared by electrodeposition of 1 g of Tl on 12 cm2 copper backings. The first plating bath is prepared by dissolving a portion of natTl2SO4 in an aqueous alkaline solution containing EDTA as a complexing agent, hydrazine hydrate as an anodic depolarizer, and Brij-35 as a wetting agent.\\n Question: What elements make up the molecules in the above context?\\n Answer: (Tl2SO4) is a molecule. Tl is the atomic symbol for thallium. S is the atomic symbol for sulfur. O is the atomic symbol for oxygen. So thallium, sulfur, and oxygen make up the molecules in the above context.\\n Context: The electrodeposition of thallium was carried out in a 100 ml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Galactica - standard (6.7b)\n",
    "# STILL TOO BIG EVEN FOR THE 4090 \n",
    "import torch\n",
    "import galai\n",
    "\n",
    "model = galai.load_model(\"standard\")\n",
    "model.generate(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc5c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am(OH)3, CaCO3, NpO2 +, PuO2 +, Np(OH)4, Pu(OH)4 Input: The solubility of NpO2 + is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56). The solubility of PuO2 + is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56). The solubility of PuO2\n"
     ]
    }
   ],
   "source": [
    "# BLOOM\n",
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=400\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe1f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3276899846a340ed9a5e4bfc175f47e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72964489ac634f0da3f8f4b0175b1c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a57286db3944c2ac8cc7631f7f3f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5155916c7a748759073c7d2b3562fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am(OH)3, CaCO3, urea Input: A major part of dissolved uranium (3.8 g of U0) is adsorbed on the surface of carbonate precipitates, with a contribution of only a little less than 2% of the U dissolved in the samples. The proportion of the U dissolved in samples containing 3.8 g of U in solution in the range 0.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT-Neo-1.3b\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
    "generator(input, do_sample=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-Neo-2.7b // this model doesn't fit on the gpu (it's like 10 gb)\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n",
    "generator(input, do_sample=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301fa29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cbe4ee6c414047b6c6a517d22e3534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)step3000/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fc39f4fa9b4e0980b5fd186e09d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5fc9c45bbf4d8f930231bd38cf0ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c63f52ff1b4622917f07b80c693927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)p3000/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0545682f6d9489dbda2f17a5924ef09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 308, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pythia-1.4b-deduped\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-1.4b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-1.4b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs, max_length=400)\n",
    "tokenizer.decode(tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pythia-2.8b-deduped\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-2.8b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-2.8b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-2.8b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-2.8b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs, max_length=400)\n",
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e1572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7867f39919e9492b8c18d481d2838cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ba94d175fc43f9884f697d0b0ad63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc833a6c55b4373ac5bbfee00adbf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600b75faac494db29621a02cc8bc00eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2db4c6bb646479bad677c0a2d23f5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccdf0a401ef4af9b33f65260112580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7646ab26689d4f57b760863868cc0d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# StableLM-tuned-alpha-3b // this is also too big to run (10.2gb)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-3b\")\n",
    "model.half().cuda()\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50278, 50279, 50277, 1, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>{input}<|ASSISTANT|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_length=400,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    ")\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
