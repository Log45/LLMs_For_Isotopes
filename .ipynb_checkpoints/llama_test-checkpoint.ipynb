{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe41a10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca449f233f244ebd911cd5626d6fa05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n\", \"comment\": { \"@type\": \"Comment\", \"author\": \"Sarah\", \"dateCreated\": \"2014-01-18T01:09:22\", \"datePublished\": \"2014-01-18T01:09:22\", \"description\": \"Hi Sarah, I'm glad you enjoyed the show. I'm glad you liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"Barry\", \"logo\": \"https://www.discover-america.co.uk/wp-content/uploads/2016/10/logo\n"
     ]
    }
   ],
   "source": [
    "# LLaMA 2,\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "model = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \\\"Breaking Bad\\\" and \\\"Band of Brothers\\\". Do you have any recommendations of other shows I might like?\\\\n', \n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f72149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80def54721f94a2191f2a28b5a8e0cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Context: After irradiation of 5 h, the 64Ni target was dissolved in 6 M hydrochloride acid, and then the solution was load to an anion exchange column to separate into different components. The 64Ni was washed out with 6 M HCl and collected for recycling. Due to the elevated cost of enriched 64Ni, recycling of the target material for re-use could reduce the production cost of 64Cu, without sacriﬁcing the quality of subsequent 64Cu production. When the eluted was switched to 1 M HCl, the ﬁrst band coming out was co-produced cobalt radioisotopes (approximately 1 mL), and the second was the 64Cu, which was collected and evaporated to dryness. The residue was dissolved in 0.1 M HCl for further use. The separation process of 64Cu took about 2.5 h after irradiation. \n",
      "\n",
      "Question: What resin/column is the solution loaded into during the above reaction? \n",
      "\n",
      "Answer: Since they say: 'the solution was load to an anion exchange column to separate into different components', and an 'anion exchange column' is a resin/column, then an anion exchange column must be the resin/column. \n",
      "\n",
      "Context: Ion-exchange separation of 203Pb. An appro- priate amount of B was dissolved in 25 mL of HCl (30% w/v). A column (1 cm i.d.) was packed with AG-1×8 resin. \n",
      "\n",
      "Question: What resin/column is the solution loaded into during the above reaction? \n",
      "\n",
      "Answer: 203Pb is a radioisotope of lead, and the reaction is an ion exchange reaction. The reaction is: \n",
      "\n",
      "\\[{\\ce {203Pb + 2HCl -> 203PbCl2 + H2}}\\]\n",
      "\n",
      "The reaction is an ion exchange reaction, and the resin/column is an ion exchange column. \n",
      "\n",
      "Context: The 64Ni target was dissolved in 6 M\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "input = \"Context: After irradiation of 5 h, the 64Ni target was dissolved in 6 M hydrochloride acid, and then the solution was load to an anion exchange column to separate into different components. The 64Ni was washed out with 6 M HCl and collected for recycling. Due to the elevated cost of enriched 64Ni, recycling of the target material for re-use could reduce the production cost of 64Cu, without sacriﬁcing the quality of subsequent 64Cu production. When the eluted was switched to 1 M HCl, the ﬁrst band coming out was co-produced cobalt radioisotopes (approximately 1 mL), and the second was the 64Cu, which was collected and evaporated to dryness. The residue was dissolved in 0.1 M HCl for further use. The separation process of 64Cu took about 2.5 h after irradiation. \\n\\nQuestion: What resin/column is the solution loaded into during the above reaction? \\n\\nAnswer: Since they say: 'the solution was load to an anion exchange column to separate into different components', and an 'anion exchange column' is a resin/column, then an anion exchange column must be the resin/column. \\n\\nContext: Ion-exchange separation of 203Pb. An appro- priate amount of B was dissolved in 25 mL of HCl (30% w/v). A column (1 cm i.d.) was packed with AG-1×8 resin. \\n\\nQuestion: What resin/column is the solution loaded into during the above reaction? \\n\\nAnswer: \"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id, load_in_8bit=True)\n",
    "\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "output = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d140f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.20-cp38-cp38-manylinux2014_x86_64.whl (109.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from xformers) (1.24.3)\n",
      "Collecting pyre-extensions==0.0.29 (from xformers)\n",
      "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from xformers) (2.0.1)\n",
      "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from pyre-extensions==0.0.29->xformers) (4.6.3)\n",
      "Requirement already satisfied: filelock in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from torch==2.0.1->xformers) (3.9.0)\n",
      "Requirement already satisfied: sympy in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from torch==2.0.1->xformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from torch==2.0.1->xformers) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from torch==2.0.1->xformers) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from jinja2->torch==2.0.1->xformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages (from sympy->torch==2.0.1->xformers) (1.2.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
      "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.29 typing-inspect-0.9.0 xformers-0.0.20\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
