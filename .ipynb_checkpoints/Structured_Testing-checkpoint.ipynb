{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117138a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for inputs\n",
    "\"\"\"\n",
    "We are going to test all of the different models in this notebook with different groups of cells representing each model\n",
    "\"\"\"\n",
    "input = \"Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86219945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 29 09:09:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:21:00.0 Off |                  N/A |\n",
      "| 35%   24C    P8    14W / 260W |     15MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1130      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1262      G   /usr/bin/gnome-shell                4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18265de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaTokenizer' from 'transformers' (/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# OpenLLaMA runs into an error trying to import LlamaTokenizer and LlamaForCausalLM, so we're moving on to the next\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# After reading I have discovered that LlamaTokenizer and LlamaForCausalLM are only obtainable \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# by getting LLaMA's weights and converting them to HuggingFace format.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer, LlamaForCausalLM\n\u001b[1;32m      7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenlm-research/open_llama_3b\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model_path = 'openlm-research/open_llama_7b'\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaTokenizer' from 'transformers' (/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# OpenLLaMA runs into an error trying to import LlamaTokenizer and LlamaForCausalLM, so we're moving on to the next\n",
    "# After reading I have discovered that LlamaTokenizer and LlamaForCausalLM are only obtainable \n",
    "# by getting LLaMA's weights and converting them to HuggingFace format.\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_length=400\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47897b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPT-2.7B\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "model = pipeline('text-generation', model=\"facebook/opt-2.7b\")\n",
    "model(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6b113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023d4b6f8d374e98a398dbafbd09263a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdbab3f5a814087b34063a72b26b97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02864bb46a9448bb818d7c64e673211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc54d69477e442fa864b97ed5a13881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am(OH)3, CaCO3, Na2CO3, K2CO3, Mg2CO3, Ca(OH)2, Mg(OH)2, Ca(OH)2, Mg(OH)2, Ca(OH)2, Mg(OH)2, Ca(OH)2, Mg(OH)2, Ca(OH)'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPT-1.3b\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "model = pipeline('text-generation', model=\"facebook/opt-1.3b\")\n",
    "model(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1f5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/generation/utils.py:1470: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Cations and anions, anions and cations, and anions and cations Answer: The most abundant elements present in the oceans are the following (with relative abundance in parenthesis): Al, As, Br, Cu, Fe, Mg, Na, Pb, S, Sr, Ti, Tl, and Zn Question: What type of molecules would be present if the ocean were a\n"
     ]
    }
   ],
   "source": [
    "# Falcon-RW-1B\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "    input,\n",
    "    max_length=400,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a7da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/home/lendes/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "['Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: AnO2 +, PuO2 +, NpO2 +, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)4, Pu(OH)']\n"
     ]
    }
   ],
   "source": [
    "# MPT-1B-Redpajama-200B-Dolly\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-1b-redpajama-200b-dolly', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('mosaicml/mpt-1b-redpajama-200b-dolly', trust_remote_code=True)\n",
    "model.to(device='cuda:0')\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\").to(device='cuda:0')\n",
    "outputs = model.generate(**inputs, max_length=400)\n",
    "text_output = tokenizer.batch_decode(outputs)\n",
    "print(text_output)\n",
    "#print(text_output[0].split(\"<|endoftext|>\")[0].split(input)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab24839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: AnO2 +, NpO2 +, PuO2 +, Pu(OH)4, Pu(OH)4 (pKsp56), Pu(OH)4 (pKsp57), Pu(OH)4 (p'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Galactica\n",
    "import torch\n",
    "import galai\n",
    "\n",
    "model = galai.load_model(\"base\")\n",
    "model.generate(input, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc5c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am(OH)3, CaCO3, NpO2 +, PuO2 +, Np(OH)4, Pu(OH)4 Input: The solubility of NpO2 + is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56). The solubility of PuO2 + is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56). The solubility of PuO2\n"
     ]
    }
   ],
   "source": [
    "# BLOOM\n",
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=400\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe1f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3276899846a340ed9a5e4bfc175f47e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72964489ac634f0da3f8f4b0175b1c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a57286db3944c2ac8cc7631f7f3f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5155916c7a748759073c7d2b3562fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am(OH)3, CaCO3, urea Input: A major part of dissolved uranium (3.8 g of U0) is adsorbed on the surface of carbonate precipitates, with a contribution of only a little less than 2% of the U dissolved in the samples. The proportion of the U dissolved in samples containing 3.8 g of U in solution in the range 0.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT-Neo-1.3b\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
    "generator(input, do_sample=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-Neo-2.7b // this model doesn't fit on the gpu (it's like 10 gb)\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n",
    "generator(input, do_sample=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301fa29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cbe4ee6c414047b6c6a517d22e3534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)step3000/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fc39f4fa9b4e0980b5fd186e09d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5fc9c45bbf4d8f930231bd38cf0ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c63f52ff1b4622917f07b80c693927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)p3000/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0545682f6d9489dbda2f17a5924ef09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/lendes/anaconda3/envs/suli/lib/python3.8/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 308, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Input: A large majority of total radioactivity has been deposited in the oceans. This includes radionuclides such as 235U, 238U, 239Pu, 137Cs, 99Tc, 90Sr, etc. Question: What radioactive elements have been deposited in the oceans? Output: Uranium-235, Uranium-238, Plutonium-239, Caesium-137, Technetium-99, Strontium-90, and more. Input: Am(OH)3 sorbs readily to surfaces. Am(III) sorption onto catcite and aragonite surfaces is both rapid and extensive, and less than 1% is readily desorbed.29 Americium(III) is more extensively sorbed onto aragonite than onto calcite, although both minerals consist of CaCO3. Question: What molecules are mentioned in the text above? Output: Am(OH)3, CaCO3 Input: The pentavalent cation AnO2 + comprises ca. 95% of the total concentration of soluble neptunium and plutonium in solutions with carbonate concentrations 10–4M. The solubility of NpO2 + may be as high as 10–4M, while the solubility of PuO2 + (10–8–10–6M), is limited by the formation of the insoluble tetravalent species Pu(OH)4 (pKsp56).  Question: What elements make up the molecules that are mentioned in the input text? Output: Am'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pythia-1.4b-deduped\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-1.4b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-1.4b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs, max_length=400)\n",
    "tokenizer.decode(tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pythia-2.8b-deduped\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-2.8b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-2.8b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-2.8b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-2.8b-deduped/step3000\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs, max_length=400)\n",
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e1572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7867f39919e9492b8c18d481d2838cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ba94d175fc43f9884f697d0b0ad63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc833a6c55b4373ac5bbfee00adbf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600b75faac494db29621a02cc8bc00eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2db4c6bb646479bad677c0a2d23f5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccdf0a401ef4af9b33f65260112580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7646ab26689d4f57b760863868cc0d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# StableLM-tuned-alpha-3b // this is also too big to run (10.2gb)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-3b\")\n",
    "model.half().cuda()\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50278, 50279, 50277, 1, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>{input}<|ASSISTANT|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_length=400,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    ")\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
